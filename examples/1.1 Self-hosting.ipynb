{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c00497",
   "metadata": {},
   "source": [
    "# Fallback models\n",
    "\n",
    "In many cases it is good to have a fallback model. There are solution like [LiteLLM](), an LLM gateway that allows you to configure accesses, fallback models, quotas and much more.\n",
    "\n",
    "If OpenAI API key is already exported, you can install `litellm[proxy]` and invoke, for example\n",
    "\n",
    "```\n",
    "litellm --model gpt-3.5-turbo\n",
    "```\n",
    "\n",
    "You will have it running immediately for testing the connection. Then, you can simply invoke the library and connect your agent to the endpoint that redirects the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=\"http://0.0.0.0:4000\", # set openai_api_base to the LiteLLM Proxy\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that im using to make a test request to.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"test from litellm. tell me why it's amazing in 1 sentence\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc7fe2",
   "metadata": {},
   "source": [
    "### Locally\n",
    "\n",
    "You can also try with locally deployed models. Using solutions like Ollama or [vLLM](https://docs.vllm.ai/en/stable/index.html) you can easily host and serve you own models. Although, machine specifications may be higher than the usual laptop, going for higher RAM and a little bit of GPU if possible.\n",
    "\n",
    "Feel free to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=128,\n",
    "    vllm_kwargs={\"quantization\": \"awq\"}, # quantized\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France ?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
