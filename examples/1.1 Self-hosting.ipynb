{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c00497",
   "metadata": {},
   "source": [
    "# Fallback models\n",
    "\n",
    "In many cases it is good to have a fallback model. There are solution like [LiteLLM](), an LLM gateway that allows you to configure accesses, fallback models, quotas and much more.\n",
    "\n",
    "If OpenAI API key is already exported, you can install `litellm[proxy]` and invoke, for example\n",
    "\n",
    "```\n",
    "litellm --model gpt-3.5-turbo\n",
    "```\n",
    "\n",
    "You will have it running immediatelly for testing the connection. Then, you can simply invoke the library and connecto your agent to the endpoint that redirects the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install litellm[proxy] langchain vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=\"http://0.0.0.0:4000\", # set openai_api_base to the LiteLLM Proxy\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that im using to make a test request to.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"test from litellm. tell me why it's amazing in 1 sentence\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc7fe2",
   "metadata": {},
   "source": [
    "### Locally\n",
    "\n",
    "You can also try with locally deployed models. Using solutions like Ollama or [vLLM](https://docs.vllm.ai/en/stable/index.html) you can easily host and serve you own models. Although, machine specifications may be higher than the usual laptop, going for higher RAM and a little bit of GPU if possible.\n",
    "\n",
    "Feel free to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b6c167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7014cbdc74f148c9a2c3a6e1c2301b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:18:21 [config.py:1604] Using max model len 4096\n",
      "INFO 07-31 08:18:22 [awq_marlin.py:120] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 07-31 08:18:22 [config.py:1084] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-31 08:18:22 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152b348bf9854bcb910938494fe741d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b643c6e618e8445380a5cc5c9f579779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3203e909144403850061f27854b578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2894aa9f24dd4e938e619eb0403e8de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc6a5d857204696b89caaa7ebda4257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:18:28 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 07-31 08:18:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='TheBloke/Llama-2-7b-Chat-AWQ', speculative_config=None, tokenizer='TheBloke/Llama-2-7b-Chat-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=TheBloke/Llama-2-7b-Chat-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-31 08:18:29 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-31 08:18:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-31 08:18:29 [gpu_model_runner.py:1843] Starting to load model TheBloke/Llama-2-7b-Chat-AWQ...\n",
      "INFO 07-31 08:18:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 07-31 08:18:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-31 08:18:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecceec1a17944c049fe293cb33d68d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:20:33 [weight_utils.py:312] Time spent downloading weights for TheBloke/Llama-2-7b-Chat-AWQ: 123.041428 seconds\n",
      "INFO 07-31 08:20:34 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452e2f5cabbb401a9fb0ab6be20f087c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:20:34 [default_loader.py:262] Loading weights took 0.76 seconds\n",
      "INFO 07-31 08:20:35 [gpu_model_runner.py:1892] Model loading took 3.6702 GiB and 125.619818 seconds\n",
      "INFO 07-31 08:20:41 [backends.py:530] Using cache directory: /home/iraitz/.cache/vllm/torch_compile_cache/1c103b7470/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-31 08:20:41 [backends.py:541] Dynamo bytecode transform time: 6.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0731 08:20:42.475000 1748476 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:20:43 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 07-31 08:20:59 [backends.py:215] Compiling a graph for dynamic shape takes 17.82 s\n",
      "INFO 07-31 08:21:14 [monitor.py:34] torch.compile takes 23.97 s in total\n",
      "INFO 07-31 08:21:15 [gpu_worker.py:255] Available KV cache memory: 2.41 GiB\n",
      "INFO 07-31 08:21:16 [kv_cache_utils.py:833] GPU KV cache size: 4,928 tokens\n",
      "INFO 07-31 08:21:16 [kv_cache_utils.py:837] Maximum concurrency for 4,096 tokens per request: 1.20x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:17<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 08:21:33 [gpu_model_runner.py:2485] Graph capturing finished in 18 secs, took 0.80 GiB\n",
      "INFO 07-31 08:21:34 [core.py:193] init engine (profile, create kv cache, warmup model) took 58.83 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b4296591ea423f9329c6358bd41227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b955b99e1f46b1b464e3cfcb649790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of France is Paris. Paris is the largest city in France and is located in the northern central part of the country. It is a major cultural, economic, and political center in Europe and is known for its iconic landmarks such as the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum.\n",
      "The capital of France has a long and complex history, with the city being part of various kingdoms, empires, and republics over the centuries. The modern city of Paris was founded in the 3rd century BC by the Gaul tribe of the Parisii, who named the\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=128,\n",
    "    vllm_kwargs={\"quantization\": \"awq\"}, # quantized\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France ?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
